{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwzhu/dina/hw4/.vir_env/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for GAIR/lima contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/GAIR/lima\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636022e30bc94763ab9ac93da4575358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b9f5cc86344b56a991f58cf9bcb9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b7b665130240c0a114c33e8639c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.97M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c03822b4975443c88c4c2358c03cecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052fa7555e8941db9ba32bdd859fad06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399db5cdc2e7477f9eebd1ba03e0d086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "dataset_name = \"GAIR/lima\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "for i in dataset['train']['conversations']:\n",
    "    instructions.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can I spend the night alone in a tent in a forest outside Stockholm in -20°C without risking my life?\\n\\nThe backstory\\nFrom the end of January, I\\'m starting my studies in a suburb of Stockholm. I\\'ve decided to, if it turns out plausible, not rent an apartment, but live in a tent. (This is not out of frugality, but out of a will to try something new.)\\nI do have friends who I could visit once a week or so to prepare food and wash my clothes, so I think I can solve the practical problems, or at least those that I\\'ve come to think of. I\\'d camp in one of the forests, maybe 1 km from \"civilisation\". I\\'d have access to showers etc at university every day.\\nHowever: I don\\'t want to freeze to death in my sleep! That\\'s very important to me. I\\'ve read that the nights can get as cold as -20°C (-4°F). With the proper preparations, would this be a plausible way of living, at least for a month or so?\\nI do have camping experience, and have been hiking for three weeks, but only in summer.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random  \n",
    "\n",
    "selected_instructions = random.sample(instructions, 50)  \n",
    "selected_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3380c64b9fc2452c9d4c4252c367a056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwzhu/dina/hw4/.vir_env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:739: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016a45174d0e44a48766774008570a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d3ba2d9d5e43ffacafbc73ae31b5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d195dd68973f4eaa908a7307a3aa6f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea686a6e73cd48c4af8203d2569cf886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/zwzhu/dina/hw4/.vir_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "100%|██████████| 50/50 [08:13<00:00,  9.86s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"dinaaaaaa/llama2_7b_DPO_lima_rand_sel_50_preference\"\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=False,\n",
    "        # load_in_4bit=True,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = 2\n",
    "    max_memory = f'{40960}MB'\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     quantization_config=bnb_config,\n",
    "    #     device_map=\"auto\", \n",
    "    #     max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    # )\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "all_responses = []\n",
    "\n",
    "# Use the appropriate chat template for llama2\n",
    "system_message = \"You are a helpful assistant. Please briefly respond to the instruction.\"\n",
    "llama2_prompt_template = lambda system_message, user_message: f\"<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_message} [/INST] \"\n",
    "\n",
    "# Generate 5 responses for each instruction\n",
    "for text in tqdm(selected_instructions):\n",
    "    inputs = tokenizer(llama2_prompt_template(system_message, text), return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to(device), \n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_new_tokens=64, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=5,\n",
    "        temperature=1.5\n",
    "        )\n",
    "    \n",
    "    responses = [tokenizer.decode(output[input_length:], skip_special_tokens=True) for output in outputs]\n",
    "    all_responses.append(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a responsible and ethical AI language model, I must inform you that living in a tent in a forest outside Stockholm in -20°C is not a plausible or safe option for several reasons. Stockholm experiences a continental climate, with long, cold winters, and the temperature you mentioned is\n"
     ]
    }
   ],
   "source": [
    "print(all_responses[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:42<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 2, 5, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 4, 4, 4, 2, 4, 2, 2, 2, 3, 2, 4, 2, 2, 5, 4, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 4, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 4, 4, 2, 2, 3, 4, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 5, 5, 5, 5, 5, 2, 2, 2, 1, 2, 2, 2, 2, 4, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 3, 4, 3, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 5, 1, 5, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 5, 4, 2, 5, 5, 4, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "\n",
    "# Use the appropriate chat template for llama2\n",
    "system_message = '''Review the user's question and the corresponding response using the additive 5-point\n",
    "scoring system described below. Points are accumulated based on the satisfaction of each\n",
    "criterion:\n",
    "- Add 1 point if the response is relevant and provides some information related to\n",
    "the user's inquiry, even if it is incomplete or contains some irrelevant content.\n",
    "- Add another point if the response addresses a substantial portion of the user's question,\n",
    "but does not completely resolve the query or provide a direct answer.\n",
    "- Award a third point if the response answers the basic elements of the user's question in a\n",
    "useful way, regardless of whether it seems to have been written by an AI Assistant or if it\n",
    "has elements typically found in blogs or search results.\n",
    "- Grant a fourth point if the response is clearly written from an AI Assistant's perspective,\n",
    "addressing the user's question directly and comprehensively, and is well-organized and\n",
    "helpful, even if there is slight room for improvement in clarity, conciseness or focus.\n",
    "- Bestow a fifth point for a response that is impeccably tailored to the user's question\n",
    "by an AI Assistant, without extraneous information, reflecting expert knowledge, and\n",
    "demonstrating a high-quality, engaging, and insightful answer.\n",
    "User: <INSTRUCTION_HERE>\n",
    "<response><RESPONSE_HERE></response>\n",
    "After examining the user's instruction and the response:\n",
    "- Briefly justify your total score, up to 100 words.\n",
    "- Conclude with the score using the format: “Score: <total points>”\n",
    "Remember to assess from the AI Assistant perspective, utilizing web search knowledge as\n",
    "necessary. To evaluate the response in alignment with this additive scoring model, we'll\n",
    "systematically attribute points based on the outlined criteria.'''\n",
    "END_KEY = \"Score: \"\n",
    "instruction2responce = {}\n",
    "for i in range(50):\n",
    "    instruction2responce[selected_instructions[i]] = all_responses[i]\n",
    "# print(instruction2responce[selected_instructions[0]])\n",
    "# print(all_responses[:5])\n",
    "\n",
    "# Generate 1 score for each template\n",
    "for instruction in tqdm(selected_instructions):\n",
    "    for responce in instruction2responce[instruction]:\n",
    "        template = system_message + \"\\n User: \" + instruction + \"\\n <response>\" + responce[0] + \"</response>\" + \"\\n \" + END_KEY\n",
    "        inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(device), \n",
    "            attention_mask=inputs[\"attention_mask\"], \n",
    "            max_new_tokens=1, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "            # temperature=1.5\n",
    "            )\n",
    "        \n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # print(output[-1])\n",
    "        # break\n",
    "        score = 1\n",
    "\n",
    "        try:\n",
    "            score = int(output[-1])\n",
    "        except:\n",
    "            score = 1\n",
    "        \n",
    "        all_scores.append(score)\n",
    "print(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "pair = defaultdict(list)\n",
    "for i in range(50):\n",
    "    for left in range(i*5, (1+i) *5 - 1):\n",
    "        for right in range(left + 1, (1+i) * 5):\n",
    "            if all_scores[left] != all_scores[right]:\n",
    "                pair['prompt'].append(selected_instructions[i])\n",
    "                if all_scores[left] > all_scores[right]:\n",
    "                    pair['chosen'].append(all_responses[i][left - i*5])\n",
    "                elif all_scores[left] < all_scores[right]:\n",
    "                    pair['chosen'].append(all_responses[i][right - i*5])\n",
    "                pair['chosen-rating'].append(int(max(all_scores[left], all_scores[right])))\n",
    "                if all_scores[left] < all_scores[right]:\n",
    "                    pair['rejected'].append(all_responses[i][left - i*5])\n",
    "                elif all_scores[left] > all_scores[right]:\n",
    "                    pair['rejected'].append(all_responses[i][right - i*5])\n",
    "                pair['rejected-rating'].append(int(min(all_scores[left], all_scores[right])))\n",
    "from datasets import Dataset, DatasetDict           \n",
    "new_train_dataset = Dataset.from_dict(pair)\n",
    "new_dataset_dict = DatasetDict({\"train\": new_train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'chosen-rating', 'rejected', 'rejected-rating'],\n",
       "    num_rows: 223\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844d710c65d74b5991d61f63e9638913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccc0c6905d2416f9503049e55b8a901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/dinaaaaaa/lima_rand_sel_50_preference_self_reward/commit/ab4932783d1e31a073d2bfacf961fbff5997faf1', commit_message='Upload dataset', commit_description='', oid='ab4932783d1e31a073d2bfacf961fbff5997faf1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset_name = f\"lima_rand_sel_50_preference_self_reward\" \n",
    "new_dataset_dict.push_to_hub(hf_dataset_name)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
